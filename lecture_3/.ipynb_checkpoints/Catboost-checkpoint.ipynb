{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we would show how one can easily with just few steps approach quite canonical competition (and dataset) hosted on widely known platform for datascience competitions [kaggle](https://www.kaggle.com). On that toy problem we would explore some base cases of using catboost, such as model training and predicting, as well as some usefull features like training visualization and model tuning.\n",
    "\n",
    "[Orginal](https://github.com/catboost/catboost/blob/master/catboost/tutorials/kaggle_titanic_catboost_demo.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from catboost import Pool, CatBoostClassifier, cv, CatboostIpythonWidget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget https://raw.githubusercontent.com/pcsanwald/kaggle-titanic/master/train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = ['pclass', 'name', 'sex', 'sibsp', 'parch', 'ticket',\n",
    "          'cabin', 'embarked']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_types = {feature: \"category\" for feature in categorical_features}\n",
    "train_df = pd.read_csv('train.csv', dtype=column_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>survived</th>\n",
       "      <th>pclass</th>\n",
       "      <th>name</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>ticket</th>\n",
       "      <th>fare</th>\n",
       "      <th>cabin</th>\n",
       "      <th>embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   survived pclass                                               name     sex  \\\n",
       "0         0      3                            Braund, Mr. Owen Harris    male   \n",
       "1         1      1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female   \n",
       "2         1      3                             Heikkinen, Miss. Laina  female   \n",
       "3         1      1       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female   \n",
       "4         0      3                           Allen, Mr. William Henry    male   \n",
       "\n",
       "    age sibsp parch            ticket     fare cabin embarked  \n",
       "0  22.0     1     0         A/5 21171   7.2500   NaN        S  \n",
       "1  38.0     1     0          PC 17599  71.2833   C85        C  \n",
       "2  26.0     0     0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3  35.0     1     0            113803  53.1000  C123        S  \n",
       "4  35.0     0     0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all lets check how many absent values do we have:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as we cat see, \"Age\", \"Cabin\" and \"Embarked\" indeed have some missing values, so lets fill them with some number way out of their distributions - so the model would be able to easily distinguish between them and take it into account:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets separate features and target variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "survived      0\n",
       "pclass        0\n",
       "name          0\n",
       "sex           0\n",
       "age         177\n",
       "sibsp         0\n",
       "parch         0\n",
       "ticket        0\n",
       "fare          0\n",
       "cabin       687\n",
       "embarked      2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.isnull().sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in (\"cabin\", \"embarked\"):\n",
    "    train_df[column].cat.add_categories((\"NA\", ), inplace=True)\n",
    "    train_df[column].fillna(\"NA\", inplace=True)\n",
    "train_df.age.fillna(-1, inplace=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_df.drop('survived', axis=1)\n",
    "y = train_df.survived\n",
    "categorical_features_indices = ((X.dtypes == \"category\").nonzero()[0])\n",
    "X[categorical_features] = X[categorical_features].apply(lambda v: v.cat.codes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pay attention that our features are of differnt types - some of them are numeric, some are categorical, and some are even just strings, which normally should be handled in some specific way (for example encoded with bag-of-words representation). But in our case we could treat these string features just as categorical one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training (CatBoost time!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, lets split our train data to train and validation sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X.values, y.values,\n",
    "                                                                train_size=0.85, random_state=1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lest create the model itself: I would go here with default parameters (as they provide a _really_ good baseline almost all the time), the only thing I'd like to specify here is `custom_loss` parameter, as this would give me an ability to see what's going on in terms of this competition metric - accuracy, as well as to be able to watch for logloss, as it would be more smooth on dataset of such size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cat_model = CatBoostClassifier(\n",
    "    custom_loss=['Accuracy'],\n",
    "    random_seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_model.fit(\n",
    "    X_train, y_train,\n",
    "    cat_features=categorical_features_indices,\n",
    "    # verbose=True,\n",
    "    # plot=False # doesn't work in Python 3\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8751438434982739"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_validation, cat_model.predict_proba(X_validation)[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Early stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have a validation set (and you should), it's always easier and better to use early stopping and make predictions for test with best model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_simple = CatBoostClassifier(\n",
    "    eval_metric='Accuracy',\n",
    "    use_best_model=False,\n",
    "    random_seed=42,\n",
    "    iterations=1000\n",
    ")\n",
    "\n",
    "model_with_earlystop = CatBoostClassifier(\n",
    "    eval_metric='Accuracy',\n",
    "    use_best_model=True,\n",
    "    random_seed=42,\n",
    "    iterations=1000\n",
    ")\n",
    "\n",
    "model_simple.fit(\n",
    "    X_train, y_train,\n",
    "    cat_features=categorical_features_indices,\n",
    "    eval_set=(X_validation, y_validation),\n",
    ")\n",
    "\n",
    "model_with_earlystop.fit(\n",
    "    X_train, y_train,\n",
    "    cat_features=categorical_features_indices,\n",
    "    eval_set=(X_validation, y_validation),\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple model validation accuracy: 0.8284\n",
      "Early-stopped model validation accuracy: 0.8433\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print('Simple model validation accuracy: {:.4}'.format(\n",
    "    accuracy_score(y_validation, model_simple.predict(X_validation))\n",
    "))\n",
    "\n",
    "print('Early-stopped model validation accuracy: {:.4}'.format(\n",
    "    accuracy_score(y_validation, model_with_earlystop.predict(X_validation))\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though as was shown earlier simple validation scheme does not precisely describes model out-of-train score (may be biased because of dataset split) it is still nice to track model improvement dynamics - and thereby as we can see from this example it is really good to stop boosting process earlier (before the overfitting kicks in)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {
    "6645a90c8fe74cb3b00687f04ffcbce8": {
     "views": [
      {
       "cell_index": 22
      }
     ]
    },
    "e39fd667552a4b25b54741c8a7d67e9a": {
     "views": [
      {
       "cell_index": 19
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
